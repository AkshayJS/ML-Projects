{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6423 : Scalable Computing Data Analytics\n",
    "\n",
    "## Assignment 1: Programming assignment for mapreduce\n",
    "\n",
    "#### Name: Akshay Javagal Somashekhar\n",
    "#### Student id: 120221054"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vfwiBpW6V4z"
   },
   "source": [
    "### **Implementing Naive Bayes in pure Python using a MapReduce Approach**\n",
    "Consider implementing a simple counting function where we sum the double of a range of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0xO0u_k961Zv"
   },
   "outputs": [],
   "source": [
    "numbers = list(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rovBazc_VxD",
    "outputId": "2996b65d-057b-46ee-dd74-d2cbda4db4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "print (numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qS7PATOy0xSm",
    "outputId": "295222f0-95b3-4d8b-b078-c5b940b10ac1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nLr0V7gF_R52"
   },
   "outputs": [],
   "source": [
    "def doubled_sum(values):\n",
    "    total = 0\n",
    "    for n in numbers:\n",
    "        total += n*2\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwvgQTps7GOx"
   },
   "source": [
    "We can print the outcome using\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QUpzAVy7RCi",
    "outputId": "ec62f9ed-189b-44f9-8792-fe2ae2fdc5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999000\n"
     ]
    }
   ],
   "source": [
    "print(doubled_sum(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCxXP0197yM1"
   },
   "source": [
    "**MapReduce version of Example Problem**\n",
    "\n",
    "As Python implements the functional programming paradigm too, it provides the functions required to implement the map-reduce paradigm builtin.\n",
    "\n",
    "The foundation tools to implement map reduce are:\n",
    "\n",
    "1.   \"mapper\" which is in charge of mapping each input value to a corresponding output value\n",
    "2.   \"reducer\" which is in charge of merging multiple mapper outputs into a single output.\n",
    "\n",
    "  \n",
    "Both phases can be called multiple times (the output of a reducer can become the input of another reducer and a mapper can call other mappers).\n",
    "\n",
    "Many MapReduce implementations also have additional phases like \"combination\" and \"aggregation\" which are executed after the mapper or the reducer to further modify their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z-cEQsqn8JlC"
   },
   "outputs": [],
   "source": [
    "numbers = list(range(1000))\n",
    "\n",
    "def mapper(value):\n",
    "    return value*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWhe0LjW2HCA",
    "outputId": "94803a7f-bfbf-458a-d5dd-a685a8729911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "print(mapper(numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EuTbqB2l2mMK"
   },
   "outputs": [],
   "source": [
    "def reducer(*values):\n",
    "    return sum(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvnFX68J8evd",
    "outputId": "fc6a7a1f-fac1-4ee8-8627-95bd2c625474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x7fde73f58d10>\n",
      "999000\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "#We can output the result of applying the map function as follows\n",
    "first_step = map(mapper, numbers)\n",
    "print (first_step)\n",
    "\n",
    "#Next we can generate the output from the reducer and print it\n",
    "result = reduce(reducer, map(mapper, numbers))\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJldU_-58Q2S"
   },
   "source": [
    "\n",
    "\n",
    "The previous map-reduce in pure python implementation lacks of course the core feature of MapReduce: working parallely.\n",
    "\n",
    "It's easy to understand that as each mapper and reducer works only on a subset of the data (its own input) it can work independently from the status of the other mappers and reducers. So the computation can proceed parallely.\n",
    "Parallel Map Reduce in Pure Python\n",
    "\n",
    "It's really easy to simulate a parallel map reduce in python by using the multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rv7nGuSCEYGU"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make numpy available using np.\n",
    "from itertools import islice\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class ParallelMapReduce(object):\n",
    "    def __init__(self, map_func, reduce_func, num_workers=None):\n",
    "        self.num_workers = num_workers\n",
    "        self.map_func = map_func\n",
    "        self.reduce_func = reduce_func\n",
    "        self.pool = multiprocessing.Pool(num_workers)\n",
    "    \n",
    "    def partition(self, n, iterable):\n",
    "        i = iter(iterable)\n",
    "        piece = list(islice(i, n))\n",
    "        while piece:\n",
    "            yield piece\n",
    "            piece = list(islice(i, n))\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        values = self.pool.map(self.map_func, inputs)\n",
    "        \n",
    "        print('>>> MAPPED VALUES (%s values): %s, ...' % (len(values), str(values[:10])))\n",
    "        values = self.pool.map(self.reduce_func, \n",
    "                               self.partition(len(values)//self.num_workers, values))\n",
    "        print('>>> REDUCED VALUES', values)\n",
    "\n",
    "        return self.reduce_func(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eH3P-8IEi6-"
   },
   "source": [
    "The previous mapreduce implementation takes a Mapper and a Reducer and splits them across num_workers until it gets back the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oN7YvHV_Em2b",
    "outputId": "07eace6c-bd68-4455-ed47-5ffe367717cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> MAPPED VALUES (11 values): [6, 8, 10, 12, 14, 16, 16, 4, 8, 10], ...\n",
      ">>> REDUCED VALUES [14, 22, 30, 20, 18, 12]\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "numbers = range(1000)\n",
    "\n",
    "def mapper(value):\n",
    "    return value*2\n",
    "\n",
    "def reducer(values):\n",
    "    return sum(values)\n",
    "\n",
    "mapreduce = ParallelMapReduce(mapper, reducer, 5)\n",
    "print(mapreduce([3,4,5,6,7,8,8,2,4,5,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cKc6yDWcnEh"
   },
   "source": [
    "Use the previous code fragments to do the following: \n",
    "\n",
    "1.   Implement a purely sequential version of Naive Bayes for Mapreduce\n",
    "2.   Implement a ParallelMapReduce version of Naive Bayes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3CLBgjAWGA8"
   },
   "source": [
    "In this project we will apply Naive Bayes to the well-known data set of mushroom data. This is found at\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/mushroom\n",
    "\n",
    "The zip-file of the data is given as part of the assignment.\n",
    "\n",
    "Data Set Information:\n",
    "\n",
    "This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\n",
    "\n",
    "The dataset consists of 8124 training examples, each representing a single mushroom. The first column is the target variable containing the class labels, identifying whether the mushroom is poisonous or edible. The remaining columns are 22 discrete features that describe the mushroom in some observable way; their values are encoded by characters. For example, gill size is either broad (b) or narrow (n), and veil color can be brown (n), orange (o), white (w), or yellow (y). Each feature has numerous values, as described below.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n",
    "2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y\n",
    "4. bruises?: bruises=t,no=f\n",
    "5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s\n",
    "6. gill-attachment: attached=a,descending=d,free=f,notched=n\n",
    "7. gill-spacing: close=c,crowded=w,distant=d\n",
    "8. gill-size: broad=b,narrow=n\n",
    "9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y\n",
    "10. stalk-shape: enlarging=e,tapering=t\n",
    "11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=?\n",
    "12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\n",
    "15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\n",
    "16. veil-type: partial=p,universal=u\n",
    "17. veil-color: brown=n,orange=o,white=w,yellow=y\n",
    "18. ring-number: none=n,one=o,two=t\n",
    "19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z\n",
    "20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y\n",
    "21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y\n",
    "22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\n",
    "\n",
    "\n",
    "The data description indicates that the feature stalk root has some missing values, denoted by ?. In this analysis, we'll read in the data and then exclude any training example that has missing values for stalk root.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0WG68Jy1ZEGw"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'\n",
    "attributes = ['mushroom_type','cap-shape','cap-surface','cap-color','bruises?','odor','gill-attachment','gill-spacing','gill-size','gill-color',\n",
    "              'stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
    "              'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat']\n",
    "data = pd.read_table(data_url, delimiter=',', header=None)\n",
    "\n",
    "#exclude any training example that has missing values for stalk root\n",
    "#data = data[data[11] != '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuykI29mh2yP"
   },
   "source": [
    "You will need to train your classifier, and then evaluate the predictive accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohdGTLdhFDQi",
    "outputId": "0c4e38cf-092d-4526-c5ba-3dc5ac0edee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial samples: 8124\n",
      "number of features: 22\n",
      "number of class labels: 2\n",
      "\n",
      "Class Labels are: ['e' 'p']\n",
      "\n",
      "Take a look at unique outcomes per feature\n",
      "0th: ['b' 'c' 'f' 'k' 's' 'x']\n",
      "1th: ['f' 'g' 's' 'y']\n",
      "2th: ['b' 'c' 'e' 'g' 'n' 'p' 'r' 'u' 'w' 'y']\n",
      "3th: ['f' 't']\n",
      "4th: ['a' 'c' 'f' 'l' 'm' 'n' 'p' 's' 'y']\n",
      "5th: ['a' 'f']\n",
      "6th: ['c' 'w']\n",
      "7th: ['b' 'n']\n",
      "8th: ['b' 'e' 'g' 'h' 'k' 'n' 'o' 'p' 'r' 'u' 'w' 'y']\n",
      "9th: ['e' 't']\n",
      "10th: ['?' 'b' 'c' 'e' 'r']\n",
      "11th: ['f' 'k' 's' 'y']\n",
      "12th: ['f' 'k' 's' 'y']\n",
      "13th: ['b' 'c' 'e' 'g' 'n' 'o' 'p' 'w' 'y']\n",
      "14th: ['b' 'c' 'e' 'g' 'n' 'o' 'p' 'w' 'y']\n",
      "15th: ['p']\n",
      "16th: ['n' 'o' 'w' 'y']\n",
      "17th: ['n' 'o' 't']\n",
      "18th: ['e' 'f' 'l' 'n' 'p']\n",
      "19th: ['b' 'h' 'k' 'n' 'o' 'r' 'u' 'w' 'y']\n",
      "20th: ['a' 'c' 'n' 's' 'v' 'y']\n",
      "21th: ['d' 'g' 'l' 'm' 'p' 'u' 'w']\n",
      "\n",
      "Remove 10th feature because it has some missing data\n",
      "Remove 15th feature because it is always 'p'\n",
      "\n",
      "After removing the two features\n",
      "number of features: 20\n",
      "\n",
      "number of training samples: 6093\n",
      "number of test samples: 2031\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#first we must do a bit of data inspection and cleaning\n",
    "y = data.iloc[:,0]\n",
    "X = data.iloc[:,1:]\n",
    "\n",
    "\n",
    "Cn=len(np.unique(y))\n",
    "\n",
    "n,d = X.shape\n",
    "\n",
    "print (\"initial samples: {}\".format(n))\n",
    "print (\"number of features: {}\".format(d))\n",
    "print (\"number of class labels: {}\".format(Cn))\n",
    "print ()\n",
    "\n",
    "print (\"Class Labels are: {}\".format(np.unique(y)))\n",
    "print ()\n",
    "\n",
    "print (\"Take a look at unique outcomes per feature\")\n",
    "for i in range(0,d):\n",
    "\tprint (\"{}th: {}\".format(i,np.unique(X.iloc[:,i])))\n",
    "\n",
    "print ()\n",
    "print (\"Remove 10th feature because it has some missing data\")\n",
    "print (\"Remove 15th feature because it is always 'p'\")\n",
    "# Dropping the 10th and 15th column as there are missing values and '?'\n",
    "X = X.drop(columns = [11,16])\n",
    "\n",
    "n,d = X.shape\n",
    "\n",
    "# dictionary master list of unique features\n",
    "featureDict = {}\n",
    "for i in range(0,d):\n",
    "\tfeatureDict[i]= np.unique(X.iloc[:,i])\n",
    "\n",
    "print ()\n",
    "print (\"After removing the two features\")\n",
    "print (\"number of features: {}\".format(d))\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 2)\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "\n",
    "print ()\n",
    "print (\"number of training samples: {}\".format(n_train))\n",
    "print (\"number of test samples: {}\".format(n_test))\n",
    "\n",
    "# Isolate the training set based on clasification label\n",
    "X_train_e = X_train[y_train=='e']\n",
    "X_train_p = X_train[y_train=='p']\n",
    "\n",
    "# capture number of each class label in training set\n",
    "n_train_e = len(X_train_e)\n",
    "n_train_p = len(X_train_p)\n",
    "\n",
    "# two dictionaries to capture likelihoods (features given class labels)\n",
    "featureGivenEd = {}\n",
    "featureGivenPo = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlPtYSdrjdsl"
   },
   "source": [
    "# Implementation of Naive Bayes algorithm using MapReduce (Series Version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXgo_Bu5jXN5",
    "outputId": "aa454acb-5ee6-4b62-cd87-9bac6fa8fbf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probability of correct prediction\n",
      "0.9960610536681438\n"
     ]
    }
   ],
   "source": [
    "# 1. Mapper Creation:\n",
    "# Compute frequencies for a feature of the given dataset\n",
    "def likelihoodMapper(feature):\n",
    "    return (feature[0],dict(feature[1].value_counts()/len(feature[1])))\n",
    "\n",
    "# 2. Reducer Creation:\n",
    "# Computes bayesian probability for a dataset.\n",
    "def reducer(testset,featureset,prob_mushroom):\n",
    "    \n",
    "    y_pred = {}\n",
    "    for index,row in testset.iterrows():\n",
    "        \n",
    "        bayesian_product_e,bayesian_product_p = 1,1\n",
    "        \n",
    "        # Iterating through each column of a test row\n",
    "        for column_name,column_value in row.items():\n",
    "            if column_value in featureset[0][column_name]:\n",
    "                \n",
    "                # Computes likelihood for each feature given the target class label 'e'\n",
    "                bayesian_product_e *= featureset[0][column_name][column_value]\n",
    "            else:\n",
    "                bayesian_product_e = 0\n",
    "                \n",
    "            if column_value in featureset[1][column_name]:\n",
    "                # Computes likelihood for each feature given the target class label 'p'\n",
    "                bayesian_product_p *= featureset[1][column_name][column_value]\n",
    "            else:\n",
    "                bayesian_product_p = 0\n",
    "                \n",
    "            if (bayesian_product_e == 0 and bayesian_product_p == 0):\n",
    "                break\n",
    "        \n",
    "        # Computes final bayesian probability for each class labels\n",
    "        bayesian_proba_e = bayesian_product_e*prob_mushroom[0]\n",
    "        bayesian_proba_p = bayesian_product_p*prob_mushroom[1]\n",
    "        \n",
    "        # Classifying the tested based on bayesian probability\n",
    "        y_pred[index] = 'e' if bayesian_proba_e > bayesian_proba_p else 'p'\n",
    "        \n",
    "    return pd.Series(y_pred)\n",
    "\n",
    "# 3. Compute the probability of mushmroom based on the target class labels \n",
    "prob_Mushroom_e = n_train_e/n_train\n",
    "prob_Mushroom_p = n_train_p/n_train\n",
    "\n",
    "# 4. Performing Map Operation for each of the features with repest to target class labels:\n",
    "# Compute frequencies of data for mushroom being edible\n",
    "featureGivenEd = dict(map(likelihoodMapper,X_train_e.iteritems()))\n",
    "# Compute frequencies of data for mushroom being poisonous\n",
    "featureGivenPo = dict(map(likelihoodMapper,X_train_p.iteritems()))\n",
    "\n",
    "# 5. Performing reducer operation\n",
    "# Reducer takes input of the testset, likelihood features given target clas labels ,probability of mushrooms given target class labels.\n",
    "# Reducer computer bayesian probabilty for each row of the test set and assigns class labels (prediction)\n",
    "y_test_pred = reducer(X_test,(featureGivenEd,featureGivenPo),(prob_Mushroom_e,prob_Mushroom_p))\n",
    "\n",
    "# 6. Performance evaluation on the test set\n",
    "print ()\n",
    "print (\"Probability of correct prediction\")\n",
    "print ((y_test_pred==y_test).sum()/n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlNHqXp9y5YQ"
   },
   "source": [
    "# Implementation of Naive Bayes algorithm using MapReduce (Parallel Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5rifNh6ezUMo"
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from itertools import islice\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "class ParallelMapReduce(object):\n",
    "    def __init__(self, map_func, reduce_func, num_workers=None):\n",
    "        self.num_workers = num_workers\n",
    "        self.map_func = map_func\n",
    "        self.reduce_func = reduce_func\n",
    "        self.data = None\n",
    "        self.X_train_e = None\n",
    "        self.X_train_p = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.prob_Mushroom_e = None\n",
    "        self.prob_Mushroom_p = None\n",
    "        self.featureGivenEd = None\n",
    "        self.featureGivenPo = None\n",
    "        self.n_train = None\n",
    "        self.n_test = None\n",
    "        self.pool = multiprocessing.Pool(num_workers)\n",
    "\n",
    "    def data_preprocess(self):\n",
    "        y = self.data.iloc[:,0]\n",
    "        X = self.data.iloc[:,1:]\n",
    "\n",
    "        n,d = X.shape\n",
    "\n",
    "        X = X.drop(columns = [11,16])\n",
    "\n",
    "        # split data into training and test sets\n",
    "        X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y,random_state = 2)\n",
    "        self.n_train = len(X_train)\n",
    "        self.n_test = len(X_test)\n",
    "\n",
    "        # Isolate the training set based on clasification label\n",
    "        self.X_train_e = X_train[y_train=='e']\n",
    "        self.X_train_p = X_train[y_train=='p']\n",
    "\n",
    "        # capture number of each class label in training set\n",
    "        n_train_e = len(X_train_e)\n",
    "        n_train_p = len(X_train_p)\n",
    "\n",
    "        # 3. Compute the probability of mushmroom based on the categorical target values \n",
    "        self.prob_Mushroom_e = n_train_e/n_train\n",
    "        self.prob_Mushroom_p = n_train_p/n_train\n",
    "\n",
    "    # Slices the dataset into sub-dataset based on number of parallel workers required\n",
    "    def partition(self, nRows, dataset):\n",
    "        i = iter(dataset.index.values.tolist())\n",
    "        # Taking slice of nrows\n",
    "        piece = list(islice(i, nRows))\n",
    "        while piece:\n",
    "            yield dataset.loc[piece]\n",
    "            piece = list(islice(i, nRows))\n",
    "\n",
    "    def __call__(self, dataset):\n",
    "        self.data = dataset\n",
    "        self.data_preprocess()\n",
    "\n",
    "        # 4. Performing Map Operation for each of the features with repest to target class labels:\n",
    "        # Compute frequencies of data for mushroom being edible\n",
    "        self.featureGivenEd = dict(self.pool.map(self.map_func, self.X_train_e.iteritems()))\n",
    "        # Compute frequencies of data for mushroom being poisonous\n",
    "        self.featureGivenPo = dict(self.pool.map(self.map_func, self.X_train_p.iteritems()))\n",
    "\n",
    "        print('>>> MAPPED VALUES:',self.featureGivenEd)\n",
    "        print('>>> MAPPED VALUES:',self.featureGivenPo)\n",
    "\n",
    "        # 5. Performing reducer operation:\n",
    "        # Reducer takes input slice of the testset, likelihood features given target clas labels ,probability of mushrooms given target class labels.\n",
    "        # Reducer computer bayesian probabilty for each row of the test set and assigns class labels (prediction)\n",
    "        reducer_partial = partial(self.reduce_func, featureset = (self.featureGivenEd,self.featureGivenPo), \n",
    "                                  prob_mushroom = (self.prob_Mushroom_e,self.prob_Mushroom_p))\n",
    "        predictions = self.pool.map(reducer_partial,self.partition(self.X_test.shape[0]//self.num_workers, self.X_test))\n",
    "        print('>>> REDUCED VALUES:',predictions)\n",
    "\n",
    "        self.y_test_pred = pd.concat(predictions)\n",
    "\n",
    "        # 6. Performance evaluation on the test set\n",
    "        print ()\n",
    "        print (\"Probability of correct prediction\")\n",
    "        accuracy = (self.y_test_pred==self.y_test).sum()/self.n_test\n",
    "        print(accuracy)\n",
    "        return self.y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLZAHv617DKT",
    "outputId": "5b602f72-5901-4118-b615-66e0680ae33c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> MAPPED VALUES: {1: {'x': 0.46528, 'f': 0.37504, 'b': 0.0992, 'k': 0.0528, 's': 0.00768}, 2: {'f': 0.36672, 'y': 0.36416, 's': 0.26912}, 3: {'n': 0.29408, 'g': 0.2432, 'w': 0.17216, 'e': 0.15328, 'y': 0.09888, 'p': 0.01312, 'b': 0.0112, 'c': 0.008, 'u': 0.00352, 'r': 0.00256}, 4: {'t': 0.6592, 'f': 0.3408}, 5: {'n': 0.80416, 'a': 0.09888, 'l': 0.09696}, 6: {'f': 0.95584, 'a': 0.04416}, 7: {'c': 0.71776, 'w': 0.28224}, 8: {'b': 0.93312, 'n': 0.06688}, 9: {'n': 0.22624, 'w': 0.21632, 'p': 0.2032, 'u': 0.10848, 'k': 0.08288, 'g': 0.06176, 'h': 0.04896, 'e': 0.02336, 'o': 0.01504, 'y': 0.01376}, 10: {'t': 0.6144, 'e': 0.3856}, 12: {'s': 0.86432, 'f': 0.09664, 'k': 0.03488, 'y': 0.00416}, 13: {'s': 0.81184, 'f': 0.10304, 'y': 0.0512, 'k': 0.03392}, 14: {'w': 0.65088, 'g': 0.14112, 'p': 0.13696, 'o': 0.04416, 'e': 0.02272, 'n': 0.00416}, 15: {'w': 0.64416, 'p': 0.13792, 'g': 0.136, 'o': 0.04416, 'e': 0.02304, 'n': 0.01472}, 17: {'w': 0.95584, 'o': 0.02336, 'n': 0.0208}, 18: {'o': 0.8752, 't': 0.1248}, 19: {'p': 0.75296, 'e': 0.23744, 'f': 0.0096}, 20: {'n': 0.42144, 'k': 0.39072, 'w': 0.13536, 'b': 0.01312, 'o': 0.01024, 'y': 0.01024, 'h': 0.0096, 'u': 0.00928}, 21: {'v': 0.27744, 'y': 0.25568, 's': 0.21216, 'n': 0.09472, 'a': 0.09152, 'c': 0.06848}, 22: {'d': 0.44544, 'g': 0.33504, 'm': 0.06304, 'l': 0.05472, 'w': 0.04576, 'p': 0.03232, 'u': 0.02368}}\n",
      ">>> MAPPED VALUES: {1: {'x': 0.4380053908355795, 'f': 0.39622641509433965, 'k': 0.15363881401617252, 'b': 0.01078167115902965, 'c': 0.0013477088948787063}, 2: {'y': 0.4467654986522911, 's': 0.3551212938005391, 'f': 0.1971024258760108, 'g': 0.0010107816711590297}, 3: {'n': 0.2543800539083558, 'e': 0.2240566037735849, 'g': 0.21057951482479784, 'y': 0.1752021563342318, 'w': 0.07816711590296496, 'b': 0.030660377358490566, 'p': 0.02324797843665768, 'c': 0.003706199460916442}, 4: {'f': 0.8416442048517521, 't': 0.158355795148248}, 5: {'f': 0.5566037735849056, 'y': 0.14824797843665768, 's': 0.14285714285714285, 'p': 0.06233153638814016, 'c': 0.04919137466307277, 'n': 0.03099730458221024, 'm': 0.00977088948787062}, 6: {'f': 0.9952830188679245, 'a': 0.0047169811320754715}, 7: {'c': 0.9723719676549866, 'w': 0.027628032345013476}, 8: {'n': 0.5606469002695418, 'b': 0.4393530997304582}, 9: {'b': 0.4356469002695418, 'p': 0.1667789757412399, 'h': 0.13847708894878707, 'g': 0.1263477088948787, 'w': 0.06435309973045822, 'n': 0.027628032345013476, 'k': 0.016172506738544475, 'u': 0.012803234501347708, 'y': 0.006064690026954178, 'r': 0.0057277628032345014}, 10: {'t': 0.512466307277628, 'e': 0.48753369272237196}, 12: {'k': 0.5758086253369272, 's': 0.3830862533692722, 'f': 0.038746630727762806, 'y': 0.0023584905660377358}, 13: {'k': 0.5491913746630728, 's': 0.39049865229110514, 'f': 0.038746630727762806, 'y': 0.0215633423180593}, 14: {'w': 0.43733153638814015, 'p': 0.3274932614555256, 'b': 0.11320754716981132, 'n': 0.10983827493261455, 'c': 0.00977088948787062, 'y': 0.0023584905660377358}, 15: {'w': 0.4191374663072776, 'p': 0.3365902964959569, 'n': 0.11657681940700809, 'b': 0.11084905660377359, 'c': 0.00977088948787062, 'y': 0.007075471698113208}, 17: {'w': 0.9976415094339622, 'y': 0.0023584905660377358}, 18: {'o': 0.9727088948787062, 't': 0.01752021563342318, 'n': 0.00977088948787062}, 19: {'e': 0.4474393530997305, 'l': 0.33524258760107817, 'p': 0.20754716981132076, 'n': 0.00977088948787062}, 20: {'w': 0.45889487870619944, 'h': 0.41206199460916443, 'k': 0.05727762803234501, 'n': 0.054245283018867926, 'r': 0.01752021563342318}, 21: {'v': 0.7240566037735849, 'y': 0.17048517520215634, 's': 0.09164420485175202, 'c': 0.013814016172506738}, 22: {'d': 0.3261455525606469, 'p': 0.25673854447439354, 'g': 0.18598382749326145, 'l': 0.1495956873315364, 'u': 0.07277628032345014, 'm': 0.00876010781671159}}\n",
      ">>> REDUCED VALUES: [606     e\n",
      "3783    p\n",
      "7701    e\n",
      "4202    p\n",
      "6956    p\n",
      "       ..\n",
      "6878    p\n",
      "6024    p\n",
      "6629    p\n",
      "7477    p\n",
      "4556    e\n",
      "Length: 203, dtype: object, 1022    e\n",
      "2284    e\n",
      "6457    p\n",
      "6147    e\n",
      "186     e\n",
      "       ..\n",
      "521     e\n",
      "5996    e\n",
      "143     e\n",
      "6700    p\n",
      "1816    p\n",
      "Length: 203, dtype: object, 7179    p\n",
      "423     e\n",
      "5301    p\n",
      "1260    p\n",
      "6357    p\n",
      "       ..\n",
      "5674    e\n",
      "2027    e\n",
      "1660    e\n",
      "4143    p\n",
      "4396    e\n",
      "Length: 203, dtype: object, 7024    p\n",
      "5106    p\n",
      "8015    e\n",
      "3320    e\n",
      "6113    p\n",
      "       ..\n",
      "5286    p\n",
      "1190    e\n",
      "1807    e\n",
      "7937    e\n",
      "7839    p\n",
      "Length: 203, dtype: object, 3593    e\n",
      "6090    p\n",
      "6096    p\n",
      "3134    e\n",
      "5550    p\n",
      "       ..\n",
      "4514    p\n",
      "5640    e\n",
      "6551    p\n",
      "7465    p\n",
      "4564    p\n",
      "Length: 203, dtype: object, 875     e\n",
      "2355    e\n",
      "7836    e\n",
      "680     e\n",
      "3244    e\n",
      "       ..\n",
      "7348    p\n",
      "7143    e\n",
      "4295    p\n",
      "2577    e\n",
      "208     e\n",
      "Length: 203, dtype: object, 4252    p\n",
      "4584    p\n",
      "2531    e\n",
      "5617    p\n",
      "3246    p\n",
      "       ..\n",
      "1400    e\n",
      "5548    p\n",
      "602     e\n",
      "7340    p\n",
      "5900    p\n",
      "Length: 203, dtype: object, 3826    p\n",
      "3423    e\n",
      "4061    e\n",
      "671     e\n",
      "2348    e\n",
      "       ..\n",
      "3276    p\n",
      "4721    p\n",
      "712     e\n",
      "2400    p\n",
      "4980    p\n",
      "Length: 203, dtype: object, 5302    e\n",
      "7614    p\n",
      "3016    p\n",
      "5485    p\n",
      "2080    e\n",
      "       ..\n",
      "2509    e\n",
      "2005    e\n",
      "2935    e\n",
      "1344    e\n",
      "6218    p\n",
      "Length: 203, dtype: object, 344     e\n",
      "1637    p\n",
      "6676    p\n",
      "5625    p\n",
      "4569    p\n",
      "       ..\n",
      "4014    p\n",
      "6538    p\n",
      "582     e\n",
      "133     e\n",
      "2323    e\n",
      "Length: 203, dtype: object, 6763    e\n",
      "dtype: object]\n",
      "\n",
      "Probability of correct prediction\n",
      "0.9960610536681438\n"
     ]
    }
   ],
   "source": [
    "# Creating a parallel mapreducer with 10 processes.\n",
    "pmr = ParallelMapReduce(likelihoodMapper, reducer, 10)\n",
    "predictions = pmr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "M5eP4kGRBMQ3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MapReduce_NaiveBayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
